{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9NNFsYAIQ47A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class RNNCell:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input-to-hidden\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden-to-hidden\n",
        "        self.bh = np.zeros((hidden_size, 1))  # Bias for hidden state\n",
        "\n",
        "        # Activation function\n",
        "        self.tanh = np.tanh\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Perform one forward pass.\n",
        "        x: input at current timestep (shape: input_size x batch_size)\n",
        "        h_prev: previous hidden state (shape: hidden_size x batch_size)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.h_prev = h_prev\n",
        "        self.h_next = self.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h_prev) + self.bh)\n",
        "        return self.h_next\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepRNN:\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        self.layers = [RNNCell(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)]\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Forward pass through the stacked RNN.\n",
        "        x: input sequence (shape: input_size x batch_size)\n",
        "        h_prev: list of previous hidden states for each layer\n",
        "        \"\"\"\n",
        "        h = h_prev  # Hidden states for all layers\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h[i] = layer.forward(x, h[i])\n",
        "            x = h[i]  # Pass hidden state to the next layer\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "-9YaTqRfQ5zR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_hidden_states(num_layers, hidden_size, batch_size):\n",
        "    return [np.zeros((hidden_size, batch_size)) for _ in range(num_layers)]\n"
      ],
      "metadata": {
        "id": "4IPK0RA3Q7IP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def train_rnn(rnn, data, targets, epochs, lr):\n",
        "    batch_size = data.shape[1]\n",
        "    hidden_size = rnn.layers[0].hidden_size\n",
        "    num_layers = len(rnn.layers)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Initialize hidden states\n",
        "        h = initialize_hidden_states(num_layers, hidden_size, batch_size)\n",
        "\n",
        "        for t in range(data.shape[0]):  # Iterate over time steps\n",
        "            x = data[t]  # Already in shape (input_size, batch_size)\n",
        "            y_true = targets[t]  # Already in shape (hidden_size, batch_size)\n",
        "\n",
        "            # Forward pass\n",
        "            h = rnn.forward(x, h)\n",
        "\n",
        "            # Loss calculation (use the output of the last layer)\n",
        "            loss = mse_loss(h[-1], y_true)\n",
        "            epoch_loss += loss\n",
        "\n",
        "\n",
        "            # Backward pass (manual gradient computation goes here)\n",
        "            # For now, skip detailed gradient implementation to keep things clear\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / data.shape[0]:.4f}\")\n"
      ],
      "metadata": {
        "id": "0jSxX46TQ-Gx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "time_steps = 10\n",
        "input_size = 3\n",
        "hidden_size = 5\n",
        "num_layers = 2\n",
        "batch_size = 2\n",
        "\n",
        "# Generate random data\n",
        "data = np.random.randn(time_steps, input_size, batch_size)\n",
        "targets = np.random.randn(time_steps, hidden_size, batch_size)\n",
        "\n",
        "# Create model\n",
        "rnn = DeepRNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Train model\n",
        "train_rnn(rnn, data, targets, epochs=5, lr=0.01)\n"
      ],
      "metadata": {
        "id": "W3ZG8qBqRAr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XY_C6HuZRnoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}